{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folders = ['art_201811', 'sports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_PERCENTILE = 90\n",
    "LARGE_PERCENTILE = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(in_folder, max_number, number_count, file_pattern, zfill_number):\n",
    "    file_numbers = numpy.random.choice(np.arange(1,max_number+1), size=number_count,replace=False)\n",
    "\n",
    "    data_array = []\n",
    "    for file_number in file_numbers:\n",
    "        file_name = os.path.join(os.getcwd(),in_folder,file_pattern+'%s.json'%(str(file_number).zfill(zfill_number)))\n",
    "                                 \n",
    "        with open(file_name,'r', encoding='utf8') as json_file:  \n",
    "            data = json.load(json_file)\n",
    "            p = {}\n",
    "            p['data'] = data['text']\n",
    "            p[\"title\"] = data[\"title\"]\n",
    "            p[\"site_section\"] = data['thread'][\"site_section\"]\n",
    "            p[\"published\"] = data[\"published\"]\n",
    "            data_array.append(p)\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# You will have to download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\TL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('names')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "english_names = nltk.corpus.names.words()\n",
    "\n",
    "def remove_names(words):\n",
    "    words_filtered = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in english_names:\n",
    "            words_filtered.append(w)\n",
    "    return words_filtered\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    words_filtered = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            words_filtered.append(w)\n",
    "    return words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(data):\n",
    "    result = []\n",
    "    #text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "    tagged_words = nltk.pos_tag(data)\n",
    "    for tagged_word in tagged_words:\n",
    "        if tagged_word[1] in ['NN','NNS']:\n",
    "            result.append(tagged_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia(keyword):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "    page_py = wiki_wiki.page(keyword)\n",
    "    if page_py.exists():\n",
    "        return (page_py.summary, page_py.categories)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_categories(cats):\n",
    "    cats_ = cats.copy()\n",
    "    list_ = ['Wiki','pages needing','accuracy disputes','redlink','category link','Pages containing'\n",
    "             ,'Articles ','Vague or ambiguous','Good articles','Articles lack','Wikipedia pages'\n",
    "             ,' symbols','Wikipedia articles','Pages with','CS','All Wikipedia articles'\n",
    "             ,'Use American','Use British English','disambiguation','Disambiguation','-language sources'\n",
    "             ,'dmy dates','mdy dates','Webarchive template','All articles','Articles containing','Articles with', \n",
    "             'Pages using','link template']\n",
    "     \n",
    "    for cat in cats:\n",
    "        if any(l_ in cat for l_ in list_):\n",
    "            cats_.pop(cat)\n",
    "    result = []\n",
    "    for cat in cats_:\n",
    "        s1 = cat.replace(\"Category:\",\"\")\n",
    "        result.append(s1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef remove_stop_categries(all_categories):\\n    cats_ = all_categories.copy()\\n    list_ = ['Wiki','pages needing','accuracy disputes','redlink','category link','Pages containing'\\n             ,'Articles ','Vague or ambiguous','Good articles','Articles lack','Wikipedia pages'\\n             ,' symbols','Wikipedia articles','Pages with','CS','All Wikipedia articles'\\n             ,'Use American','Use British English','disambiguation','Disambiguation','-language sources'\\n             ,'dmy dates','mdy dates','Webarchive template','All articles','Articles containing','Articles with' \\n             ,'Pages using','link template','time', 'calendar', 'Calendar', 'date', 'month','day of', 'Year', 'Units of'\\n             ,'Customary units', 'Integer','Punctuation', 'All self-contradictory articles', 'Weeks'\\n            ]\\n\\n    for cat in all_categories:\\n        if any(l_ in cat for l_ in list_):\\n            cats_.remove(cat)\\n    return cats_\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_categories(data_array):\n",
    "    set_categories = set()\n",
    "    all_categories = []\n",
    "    for data in data_array:\n",
    "        for cat in data['categories']:\n",
    "            for c in cat:\n",
    "                set_categories.add(c)\n",
    "                all_categories.append(c)\n",
    "            \n",
    "    return all_categories\n",
    "\n",
    "\"\"\"\n",
    "def remove_stop_categries(all_categories):\n",
    "    cats_ = all_categories.copy()\n",
    "    list_ = ['Wiki','pages needing','accuracy disputes','redlink','category link','Pages containing'\n",
    "             ,'Articles ','Vague or ambiguous','Good articles','Articles lack','Wikipedia pages'\n",
    "             ,' symbols','Wikipedia articles','Pages with','CS','All Wikipedia articles'\n",
    "             ,'Use American','Use British English','disambiguation','Disambiguation','-language sources'\n",
    "             ,'dmy dates','mdy dates','Webarchive template','All articles','Articles containing','Articles with' \n",
    "             ,'Pages using','link template','time', 'calendar', 'Calendar', 'date', 'month','day of', 'Year', 'Units of'\n",
    "             ,'Customary units', 'Integer','Punctuation', 'All self-contradictory articles', 'Weeks'\n",
    "            ]\n",
    "\n",
    "    for cat in all_categories:\n",
    "        if any(l_ in cat for l_ in list_):\n",
    "            cats_.remove(cat)\n",
    "    return cats_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_create_folder(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_categories(data_array, in_folder):\n",
    "    for count in range(3,101):\n",
    "        data = data_array[count]\n",
    "        print(count)\n",
    "        data['categories'] = []\n",
    "        sentenses = split_into_sentences(data['data'])\n",
    "        for sentense in sentenses:\n",
    "            words = word_tokenize(sentense)\n",
    "            words = remove_names(words)\n",
    "            words = remove_stop_words(words)\n",
    "            nouns = get_nouns(words)\n",
    "            categories = []\n",
    "            for noun in nouns:\n",
    "                wiki = get_wikipedia(noun[0])\n",
    "                if wiki:\n",
    "                    cats = filter_categories(wiki[1])\n",
    "                    data['categories'].append(cats)\n",
    "        check_create_folder(os.path.join(os.getcwd(), in_folder,'json'))\n",
    "        with open(os.path.join(os.getcwd(), in_folder,'json','data_array%s.json'%(str(count))), 'w') as fout:\n",
    "            json.dump(data , fout)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_data_array(data_array):\n",
    "    max_value = list(data_array['class'][0].values())[0]\n",
    "    cnt = 0\n",
    "    items = []\n",
    "    for item in data_array['class']:\n",
    "        if max_value > list(item.values())[0]:\n",
    "            max_value = list(item.values())[0]\n",
    "            cnt += 1\n",
    "            if cnt > 2:\n",
    "                data_array['class'] = items\n",
    "                return data_array\n",
    "        items.append(item)    \n",
    "        #print(item)\n",
    "    return data_array     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_results(data_array, in_folder, keyword):\n",
    "    allsyns_array = []\n",
    "    for i in range(0,len(data_array)):\n",
    "        data_array[i] = prune_data_array(data_array[i])\n",
    "        word_list = [list(item.keys())[0] for item in data_array[i]['class']]\n",
    "\n",
    "        allsyns1 = set(ss for word in word_list for ss in wordnet.synsets(word, pos=wordnet.NOUN))\n",
    "        if len(allsyns1) > 0:\n",
    "            allsyns2 = set(wordnet.synsets(keyword, pos=wordnet.NOUN))\n",
    "            best = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in \n",
    "                product(allsyns1, allsyns2))\n",
    "            worst = min((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in \n",
    "                product(allsyns1, allsyns2))\n",
    "            print((best[0]+worst[0])/2)\n",
    "            data = {'data':data_array[i]['data'],'best':best[0], 'worst':worst[0]}\n",
    "            allsyns_array.append(data)\n",
    "            check_create_folder(os.path.join(os.getcwd(),in_folder,'3level_theme'))\n",
    "            with open(os.path.join(os.getcwd(),in_folder,'3level_theme','data_array%s.json'%(str(i))), 'w') as fout:\n",
    "                json.dump(data , fout)   \n",
    "    return allsyns_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_array_business = get_data('business', 14794, 1000, 'news_00', 5)\n",
    "\n",
    "data_array_art = get_data('art_201811', 100, 100, 'post_', 3)\n",
    "\n",
    "len(data_array_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_create_folder(os.path.join(os.getcwd(), 'art_201811','json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "data_array_art = add_categories(data_array_art, 'art_201811')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsyns_array = get_classification_results(data_array_art, 'art_201811', 'science')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
